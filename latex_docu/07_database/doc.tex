\section{Database}
%
\subsection{Overview}
%
%
\subsection{Requirements}
\subsubsection{Requirements}
\begin{itemize}  
\item scale-able in the range of petabyte in size
\item hundred of thousands of requests per minute
\item high availablility
\item partition tolerance
\item fast to handle timeseries
\item fast to handle geolocation data
\item  immediate consistency is NOT necessary
\end{itemize}
%
\subsubsection{Requirements}
%
\begin{itemize}  
\item Open source or at the very least an open license is required
\item must be well documented
\item must be managable regarding administration and learning afford
\end{itemize}
\subsection{Survey of Existing Solutions}
%
%
\subsection{Evaluation Criteria and Decision-making Process}
The process of deciding what database architecture to use we started with our requirements. 

Espacialy the last point of our secondary requiremnts had to be taken into account, because we had no real database expert in our team, so we first considert database-system we allready knew.
Our approach was to check whether those databases fulfill our requirements first.

Any relational database as main datastorage has been quickly disreagarded, cause of the bad fairly scaling behavir with the amount of data we have to handly.
\subsection{Implementation Details}
\subsubsection{Intro to Elasticsearch}

Elasticsearch is an opensource Lucene based search engine. It is under active development, with an extensive documentation.
\subsubsection{Architecture}
Each index can be sharded and each shard can have multiple indieces.\\

TODO: Picture of architecture\\
\\
To better distribute search requests, the workload is divided among all shards belonging to an index. Because that
would not scale very well and would have no partition tolerance,
each shard has a configurable number of replicas. A new search request is send to on replica of each shard.
\subsubsection{Data model}
We decided to have an data model which is data-source-centric with the extra posibility to partition the data over time. 
That means, each data source gets it own index with its own timeframe and its own adjusted datastructure. 
All our data sources save a few basic data point with each element stored in the database, in particular are those:
\begin{itemize}  
\item timestamp: when has the datapoint been recorded
\item location: where has the datapoint been recorded
\end{itemize}
Those are acutally the only information we need to store, besides the individual measurements. We do acutally store some more information, 
but regarding the common usecases for searches those two datapoints are enough for environemental data. Please refer to the full data-model in the appedix for more information.\\
\\
This data-model has multiple advantages:
\begin{itemize}  
\item it keeps the data provenance
\item it allows us to adjust the server infrastructe based on the data source
\item it scales indefinitely
\item index size is deterministic, cause of time based partitioning
\end{itemize}
So why does it scale so good? When importing data from one source, I process and store the data points in one index. This index is not just limited to the data source,
it is also limited to the time, e.g. 2016. That means, when 2016 is finished with importing data, the index is done and can be closed up, no one needs to care about it anymore.
After the index is done, it might even be transfered to another Elasticsearch node with different hardware.
That would be usefull, for example, when the average density of the smurf population is beeing stored. 
The index can be transferd to a less powerfull hardware with fewer CPU cores and spinning harddrives and even fewer replicas, 
because this information is probably hardly requested.
%
\subsubsection{Query optimization}
A search request in send to all indieces, that means:

Why do we need query optimization? For that I'm going to give a small small example to consider:
\begin{enumerate}  
\item we import multiple sources, with multiple messurements: source1(airtemperature, watertemperatur) 1980-2017, source2(airtemperature) 1983-1990, source3(uv-index) 2009-2017
\item each source is partitioned by year and source 1 is partitioned by month for all data after 2015.
\item every index is naivly sharded over 3 nodes
\end{enumerate}
Let's make a simple search request: give me all uv values data from 2015 till 2017 and aggregate an everage over the month.\\
Because the user does not now anything about the internal database architecture (at least he should not) he requests the temperature and the timeframe.\\
\\
\textbf{Worst case:}

\label{eq:jOliver}
\begin{align}
source1 = 35 years + (2years x 12 month) x 3 shards
\end{align}
\begin{align}
source1 = 177 shards
\end{align}
\begin{align}
source2 = 17 years x 3 shars
\end{align}
\begin{align}
source2 = 51 shards
\end{align}
\begin{align}
source3 = 8 years x 3 shars
\end{align}
\begin{align}
source3 = 8 years x 3 shars
\end{align}
\begin{align}
source1 + source2 + source3 = 252 shards
\end{align}
So in worst case each shard has its own node(very unlikely), the search request has to be send to 252 nodes/computers.\\
\\
\textbf{First optimization, Limit the time}\\
\\
With a naive approach by checking the common time part of the request 2016-2017 and limit the indieces search with the following pattern:
\begin{align}
indexsearch: *-201*
\end{align}
\begin{align}
source1 = 5 years + (2years x 12 month) x 3 shards
\end{align}
\begin{align}
source1 = 87 shards
\end{align}
\begin{align}
source2 = 0 years x 3 shars
\end{align}
\begin{align}
source2 = 0 shards
\end{align}
\begin{align}
source3 = 3 years x 3 shars
\end{align}
\begin{align}
source3 = 9 shards
\end{align}
\begin{align}
source1 + source2 + source3 = 96 shards
\end{align}
We allready reduced the number of shards we need to address by 61\%.\\
\\
TODO:!!!
\\
With a little more sophisticated timelimitation algorithm, we could acctually do more and search just those two years:
If we store in a seperate database, which data source and therefore indiece actually holds the requested data we can do even much more:\\
\\
Now we are at 68\% reduction.\\
\\
\textbf{Second optimization, Limit to indieces which contain the right data}\\
\\
If we store in a seperate database, which data source and therefore indiece actually holds the requested data we can do even much more:\\
\\
TODO\\
\\
By using those two optimitzations, we were able to reduce the number of requeseted shard to 6, which means a total reduction of 96.2\%.\\
\\
This was just a naive example. In reality the reduction should even be much higher, with a growing number of data sources.
Let's say we have allready 100 data sources and we can limit a request to just two of those for example because the requested messuremnt it
provided just by those two, the saving of network traffic and workload would be immense.
\subsection{Critical Analysis/Limitations}
\subsubsection{Joins}
One mayor drawback of elasticsearch is the missing possibility of server side join, the way they are known by SQL based database-system. 
This means, any kind of join operation has to be done either on a seperate server, like our api instance, or on the application side. 
This is actually something we were not really aware of for a long time.